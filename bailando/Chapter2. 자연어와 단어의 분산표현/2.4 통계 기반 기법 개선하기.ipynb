{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 통계 기반 기법 개선하기\n",
    "앞 절에서 단어의 동시발생 행렬을 통해 단어를 벡터로 만들었으나, 이것 개선해보자"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.1 상호정보량\n",
    "동시발생 행렬의 원소는 두 단어가 동시에 발생한 횟수를 나타낸다.  \n",
    "하지만 '발생'횟수는 좋은 특징이 이낟. 고빈도 단어를 보면 알수 있는데 말뭉치에서 'the'나 'car'의 동시발생을 보면 두 단어는 동시발생확률이 매우 높다. 하지만 the car보다는 car drive가 더 관련성이 강하다.   \n",
    "이 문제를 해결하기 위해서 `점별 상호정보량(PMI)`라는 척도를 사용한다.\n",
    "\n",
    "$$PMI(x,y)=log_2 \\frac{P(x,y)}{P(x)P(y)}$$\n",
    "\n",
    "PMI를 통하면 아까와 같은 문제를 해결할 수 있다.  \n",
    "전체 말뭉치의 단어수(N)이 10,000일 때, 'the'가 1000번, 'car'가 20번, 'drive'가 10번 발생했고, the car는 10번 동시발생, car drive는 5회라고 가정한다.\n",
    "이 때, the 와 car의 PMI는 2.32, car와 drive의 PMI는 7.97로 원하는 척도를 얻을 수 있다.\n",
    "\n",
    "PMI에 한 가지 문제가 있는데, 바로 두 단어의 동시발생 횟수가 0이면 $log_20 = - \\infty$ 가 된다는 점이다. 이 문제를 피하기 위해 실제로 구현할 때에는 양의 상호정보량(PPMI: Positive PMI)를 사용한다.\n",
    "\n",
    "$$PPMI(x,y)=max(0, PMI(x,y))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def ppmi(C, verbose=False, eps=1e-8) -> list:\n",
    "    \"\"\"Make ppmi from 동시발생 행렬\n",
    "\n",
    "    Args:\n",
    "        C (list): 동시발생행렬\n",
    "        verbose (bool, optional): 상세히 출력하기 위한 옵션. Defaults to False.\n",
    "        eps (float, optional): log2가 음의 무한대가 되는 것을 막기 위한 임의의 작은 수. Defaults to 1e-8.\n",
    "\n",
    "    Returns:\n",
    "        list: ppmi\n",
    "    \"\"\"\n",
    "    M = np.zeros_like(C, dtype=np.float32)\n",
    "    N = np.sum(C)\n",
    "    S = np.sum(C, axis=0)\n",
    "    total = C.shape[0] * C.shape[1]\n",
    "    cnt = 0\n",
    "\n",
    "    for i in range(C.shape[0]):\n",
    "        for j in range(C.shape[1]):\n",
    "            pmi = np.log2(C[i, j] * N / (S[j] * S[i]) + eps)\n",
    "            M[i, j] = max(0, pmi)\n",
    "\n",
    "            if verbose:\n",
    "                cnt += 1\n",
    "                if cnt % (total//100 + 1) == 0:\n",
    "                    print(f\"{100*cnt/total:.1f} 완료\")\n",
    "    return M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "동시발생 행렬\n",
      "[[0 1 0 0 0 0 0]\n",
      " [1 0 1 0 1 1 0]\n",
      " [0 1 0 1 0 0 0]\n",
      " [0 0 1 0 1 0 0]\n",
      " [0 1 0 1 0 0 0]\n",
      " [0 1 0 0 0 0 1]\n",
      " [0 0 0 0 0 1 0]]\n",
      "--------------------------------------------------\n",
      "PPMI\n",
      "[[0.    1.807 0.    0.    0.    0.    0.   ]\n",
      " [1.807 0.    0.807 0.    0.807 0.807 0.   ]\n",
      " [0.    0.807 0.    1.807 0.    0.    0.   ]\n",
      " [0.    0.    1.807 0.    1.807 0.    0.   ]\n",
      " [0.    0.807 0.    1.807 0.    0.    0.   ]\n",
      " [0.    0.807 0.    0.    0.    0.    2.807]\n",
      " [0.    0.    0.    0.    0.    2.807 0.   ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from utils import preprocess, create_co_matrix, cos_similarity, ppmi\n",
    "\n",
    "text = 'You say goodbye and I say hello.'\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "vocab_size = len(word_to_id)\n",
    "C = create_co_matrix(corpus, vocab_size)\n",
    "W = ppmi(C)\n",
    "\n",
    "np.set_printoptions(precision=3)\n",
    "print('동시발생 행렬')\n",
    "print(C)\n",
    "print('-'*50)\n",
    "print('PPMI')\n",
    "print(W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "동시발생을 PPMI 행렬로 변환해보았다. PPMI 행렬의 각 원소는 모두 0 이상의 실수이다.  \n",
    "이제 우리는 더 좋은 척도로 이뤄진 행렬(더 좋은 단어 벡터)을 손에 쥐었습니다.\n",
    "\n",
    "그러나 PPMI에도 여전히 문제가 있다. 말뭉치의 어휘수가 증가함에 따라 각 단어벡터의 차원 수도 증가한다는 문제다. 예를 들어 말뭉치의 어휘 수가 10만 개라면 그 벡터의 차원수도 똑같이 10만이 된다. 10만 차원의 벡터를 다룬다는 것은 그다지 현실적이지 않다.\n",
    "이 문제에 대처하고자 자주 수행하는 기법이 바로 벡터의 차원 감소이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.2 차원 감소\n",
    "차원감소는 문자 그대로 벡터의 차원을 줄이는 방법을 말한다. 그러나 단순히 줄이는 것이 아니라 '중요한 정보'는 최대한 유지하면서 줄이는 게 핵심이다.  \n",
    "아래 사진 처럼 데이터의 분포를 고려해 중요한 '축'을 찾는 일을 말한다.\n",
    "\n",
    "![](https://github.com/yesinkim/Deep-Learning-From-Scratch2/blob/main/deep_learning_2_images/fig%202-8.png?raw=true)\n",
    "왼쪽은 데이터점들을 2차원 좌표에 표시한 모습이고, 오른쪽은 새로운 축을 도입하여 똑같은 데이터를 좌표축 하나만으로 표시했다. 이 때 각 데이터점의 값은 새로운 축으로 사영된 값으로 변한다. 여기서 중요한 것은 가장 적합한 축을 찾아내는 일로, 1차원 값만으로도 데이터의 본질적인 차이를 구별할 수 있어야 한다.\n",
    "\n",
    "> NOTE: 원소 대부분이 0인 행렬 또는 벡터를 희소(Sparse)행렬 및 희소벡터라고 한다. 차원 감소의 핵심은 희소벡터에서 중요한 축을 찾아내 더 적은 밀집벡터로 변환하는 것이다. 이 조밀한 벡터야말로 우리가 원하는 단어의 분산표현이다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVD(특잇값 분해)\n",
    "\n",
    "차원을 감소시키는 방법은 여러 가지이지만, 우리는 **특잇값분해(SVD:Singular Value Decomposition)** 를 이용한다.  \n",
    "SVD는 임의의 행렬을 세 행렬의 곱으로 분해하며, 수식으로는 다음과 같다\n",
    "$$X = USV^T$$\n",
    "SVD는 임의의 행렬 X를 U, S, V라는 세 행렬의 곱으로 분해한다.\n",
    "U와 V는 직교행렬이고, 그 열 벡터는 서로 직교한다. 또한 S는 대각행렬(대각성분 외에는 모두 0인 행렬)이다. (??뭔말)\n",
    "![](https://github.com/yesinkim/Deep-Learning-From-Scratch2/blob/main/deep_learning_2_images/fig%202-9.png?raw=true)\n",
    "$U$는 직교행렬이다. 이 직교행렬은 어떠한 축(기저)을 형성한다. 지금 우리의 맥락에서는 이 $U$행렬을 '단어 공간'으로 취급할 수 있고, $S$는 대각행렬로 '특잇값'이 큰 순서로 나열되어 있다. 특잇값이란 '해당 축'의 중요도라고 간주할 수 있다. 아래 사진처럼 중요도가 낮은 원소를 깎아내는 방법을 생각할 수 있다.\n",
    "![](https://github.com/yesinkim/Deep-Learning-From-Scratch2/blob/main/deep_learning_2_images/fig%202-10.png?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 0 0 0 0]\n",
      "[0.    1.807 0.    0.    0.    0.    0.   ]\n",
      "[-3.409e-01 -1.110e-16 -4.441e-16  1.205e-01  9.323e-01  0.000e+00\n",
      "  3.207e-16]\n"
     ]
    }
   ],
   "source": [
    "from utils import preprocess, create_co_matrix, ppmi\n",
    "\n",
    "text = 'You say goodbye and I say hello.'\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "vocab_size = len(id_to_word)\n",
    "C = create_co_matrix(corpus, vocab_size, window_size=1)\n",
    "W = ppmi(C)\n",
    "\n",
    "# SVD\n",
    "U, S, V = np.linalg.svd(W)\n",
    "print(C[0])     # 동시발생 행렬\n",
    "print(W[0])     # PPMI 행렬\n",
    "print(U[0])     # SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 결과에서 보듯 원래는 희소벡터는 W[0]이 SVD를 통해 밀집벡터 U[0]로 변했다. 그리고 이 밀집벡터의 차원을 감소시키려면, 단순히 처음의 두 원소를 꺼내면 된다(중요한 순서로 정렬되어 있기 때문에)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3.409e-01 -1.110e-16]\n"
     ]
    }
   ],
   "source": [
    "print(U[0, :2])     # 쫌 이상함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 각 단어를 2차원 벡터로 표편한 후 그래프로 그려본다        --> 안나타나는데..\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# for word, word_id in word_to_id.items():\n",
    "#     plt.annotate(word, U[word_id, 0], U[word_id, 1])\n",
    "\n",
    "# plt.scatter(U[:,0], U[:,1], alpha=0.5)\n",
    "# plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://github.com/yesinkim/Deep-Learning-From-Scratch2/blob/main/deep_learning_2_images/fig%202-11.png?raw=true)\n",
    "이 그림을 보면 'goodbye'와 'hello', 그리고 'you'와 'i'가 제법 가까이 있어 우리의 직관과 비슷해진 것을 볼 수 있다.\n",
    "하지만 너무 작은 말뭉치를 사용했기 때문에 결과가 석연치 않다. PTB데이터 셋이라는 더 큰 말뭉치를 이용해서 똑같은 실험을 진행해보자.\n",
    "\n",
    "> WARNING: 행렬의 크기가 N이면 SVD 계산은 O(N^3)이 걸린다. 이는 현실적으로 감당하기 어려운 수준이므로 Truncated SVD같은 더 빠른 기법을 이용한다. 이것은 특잇값이 작은 것은 버리는 방식으로 성능 향상을 꾀한다. 다음 절에서도 skicit-learn 의 truncated SVD를 이용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.4 PTD 데이터 셋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "말뭉치 크기: 929589\n",
      "corpus[:30]: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29]\n",
      "\n",
      "id_to_word[0]: aer\n",
      "id_to_word[1]: banknote\n",
      "id_to_word[2]: berlitz\n",
      "\n",
      "word_to_id['car']: 3856\n",
      "word_to_id['happy']: 4428\n",
      "word_to_id['lexus']: 7426\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import ptb\n",
    "\n",
    "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
    "\n",
    "print('말뭉치 크기:', len(corpus))\n",
    "print('corpus[:30]:', corpus[:30])\n",
    "print()\n",
    "print('id_to_word[0]:', id_to_word[0])\n",
    "print('id_to_word[1]:', id_to_word[1])\n",
    "print('id_to_word[2]:', id_to_word[2])\n",
    "print()\n",
    "print(\"word_to_id['car']:\", word_to_id['car'])\n",
    "print(\"word_to_id['happy']:\", word_to_id['happy'])\n",
    "print(\"word_to_id['lexus']:\", word_to_id['lexus'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.5 PTB 데이터셋 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "동시발생 수 계산\n",
      "PPMI 계산\n",
      "1.0 완료\n",
      "2.0 완료\n",
      "3.0 완료\n",
      "4.0 완료\n",
      "5.0 완료\n",
      "6.0 완료\n",
      "7.0 완료\n",
      "8.0 완료\n",
      "9.0 완료\n",
      "10.0 완료\n",
      "11.0 완료\n",
      "12.0 완료\n",
      "13.0 완료\n",
      "14.0 완료\n",
      "15.0 완료\n",
      "16.0 완료\n",
      "17.0 완료\n",
      "18.0 완료\n",
      "19.0 완료\n",
      "20.0 완료\n",
      "21.0 완료\n",
      "22.0 완료\n",
      "23.0 완료\n",
      "24.0 완료\n",
      "25.0 완료\n",
      "26.0 완료\n",
      "27.0 완료\n",
      "28.0 완료\n",
      "29.0 완료\n",
      "30.0 완료\n",
      "31.0 완료\n",
      "32.0 완료\n",
      "33.0 완료\n",
      "34.0 완료\n",
      "35.0 완료\n",
      "36.0 완료\n",
      "37.0 완료\n",
      "38.0 완료\n",
      "39.0 완료\n",
      "40.0 완료\n",
      "41.0 완료\n",
      "42.0 완료\n",
      "43.0 완료\n",
      "44.0 완료\n",
      "45.0 완료\n",
      "46.0 완료\n",
      "47.0 완료\n",
      "48.0 완료\n",
      "49.0 완료\n",
      "50.0 완료\n",
      "51.0 완료\n",
      "52.0 완료\n",
      "53.0 완료\n",
      "54.0 완료\n",
      "55.0 완료\n",
      "56.0 완료\n",
      "57.0 완료\n",
      "58.0 완료\n",
      "59.0 완료\n",
      "60.0 완료\n",
      "61.0 완료\n",
      "62.0 완료\n",
      "63.0 완료\n",
      "64.0 완료\n",
      "65.0 완료\n",
      "66.0 완료\n",
      "67.0 완료\n",
      "68.0 완료\n",
      "69.0 완료\n",
      "70.0 완료\n",
      "71.0 완료\n",
      "72.0 완료\n",
      "73.0 완료\n",
      "74.0 완료\n",
      "75.0 완료\n",
      "76.0 완료\n",
      "77.0 완료\n",
      "78.0 완료\n",
      "79.0 완료\n",
      "80.0 완료\n",
      "81.0 완료\n",
      "82.0 완료\n",
      "83.0 완료\n",
      "84.0 완료\n",
      "85.0 완료\n",
      "86.0 완료\n",
      "87.0 완료\n"
     ]
    }
   ],
   "source": [
    "from utils import create_co_matrix, most_similar\n",
    "window_size = 2\n",
    "wordvec_size = 100\n",
    "\n",
    "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
    "vocab_size = len(word_to_id)\n",
    "print('동시발생 수 계산')\n",
    "C = create_co_matrix(corpus, vocab_size, window_size)\n",
    "print('PPMI 계산')  # 이런거 왜 쓰는지?ㅋㅋ\n",
    "W = ppmi(C, verbose=True)\n",
    "\n",
    "print('SVD 계산')\n",
    "try: \n",
    "    # truncate SVD\n",
    "    from sklearn.utils.extmath import randomized_svd\n",
    "    U, S, V = randomized_svd(W, n_components=wordvec_size, n_iter=5,\n",
    "                             random_state=None)\n",
    "except ImportError:\n",
    "    U, S, V = np.linalg.svd(W)\n",
    "\n",
    "word_vecs = U[:, :wordvec_size]\n",
    "\n",
    "querys = ['you', 'year', 'car', 'toyota']\n",
    "for query in querys:\n",
    "    most_similar(query, word_to_id, id_to_word, word_vecs, top=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('estud')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "391b1e7ce2e81530d6fa13507e2cf4fd1c09c0aaf5e54d082942efacf0dd49e1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
